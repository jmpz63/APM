#!/usr/bin/env python3
"""
Enhanced AI Knowledge Workflow Automation System
Automatically handles knowledge acquisition, learning, documentation, indexing, and pushing

This system ensures AI agents always know when new information is acquired and 
automatically processes it through the complete knowledge lifecycle.

Usage:
    python3 ai_knowledge_workflow.py --mode [auto|manual|monitor]
    
Features:
- Automatic knowledge detection and acquisition
- Intelligent learning and synthesis 
- Comprehensive documentation generation
- Index updates and maintenance
- Automated git commit and push workflows
- Knowledge quality validation
- Integration status tracking
"""

import os
import json
import subprocess
import hashlib
import time
from pathlib import Path
from datetime import datetime, timezone
import argparse
import shutil

class AIKnowledgeWorkflow:
    def __init__(self):
        self.apm_root = Path("/home/arm1/APM")
        self.knowledge_base = self.apm_root / "Knowledge_Base"
        self.admin_dir = self.apm_root / "_ADMIN"
        self.workflow_state_file = self.admin_dir / "knowledge_workflow_state.json"
        self.master_index = self.apm_root / "00_MASTER_KNOWLEDGE_INDEX.md"
        self.expansion_index = self.knowledge_base / "KNOWLEDGE_EXPANSION_INDEX.md"
        
        # Knowledge tracking
        self.knowledge_state = self.load_workflow_state()
        self.new_knowledge_detected = False
        self.integration_queue = []
        
    def load_workflow_state(self):
        """Load the current workflow state"""
        if self.workflow_state_file.exists():
            with open(self.workflow_state_file, 'r') as f:
                return json.load(f)
        else:
            return {
                "last_scan_timestamp": 0,
                "file_checksums": {},
                "knowledge_items": [],
                "integration_status": "idle",
                "last_push_timestamp": 0,
                "pending_integrations": [],
                "auto_workflow_enabled": True
            }
    
    def save_workflow_state(self):
        """Save the current workflow state"""
        self.knowledge_state["last_scan_timestamp"] = datetime.now(timezone.utc).timestamp()
        with open(self.workflow_state_file, 'w') as f:
            json.dump(self.knowledge_state, f, indent=2)
    
    def detect_new_knowledge(self):
        """Detect new knowledge files and changes"""
        print("üîç Scanning for new knowledge acquisition...")
        
        # Scan knowledge directories for changes
        knowledge_dirs = [
            self.knowledge_base,
            self.apm_root / "Projects",
            self.apm_root / "Research", 
            self.apm_root / "Learning"
        ]
        
        current_files = {}
        new_files = []
        modified_files = []
        
        for directory in knowledge_dirs:
            if directory.exists():
                for file_path in directory.rglob("*.md"):
                    if file_path.is_file():
                        # Calculate file checksum
                        with open(file_path, 'rb') as f:
                            checksum = hashlib.md5(f.read()).hexdigest()
                        
                        relative_path = str(file_path.relative_to(self.apm_root))
                        current_files[relative_path] = {
                            "checksum": checksum,
                            "timestamp": file_path.stat().st_mtime,
                            "size": file_path.stat().st_size
                        }
                        
                        # Check if new or modified
                        if relative_path not in self.knowledge_state["file_checksums"]:
                            new_files.append(relative_path)
                            self.new_knowledge_detected = True
                        elif self.knowledge_state["file_checksums"][relative_path]["checksum"] != checksum:
                            modified_files.append(relative_path)
                            self.new_knowledge_detected = True
        
        # Update file checksums
        self.knowledge_state["file_checksums"] = current_files
        
        if new_files:
            print(f"üìÑ Detected {len(new_files)} new knowledge files:")
            for file in new_files:
                print(f"   ‚Ä¢ {file}")
        
        if modified_files:
            print(f"üìù Detected {len(modified_files)} modified knowledge files:")
            for file in modified_files:
                print(f"   ‚Ä¢ {file}")
        
        return new_files, modified_files
    
    def analyze_knowledge_content(self, file_paths):
        """Analyze new knowledge content for automatic categorization"""
        print("üß† Analyzing knowledge content...")
        
        analysis_results = []
        
        for file_path in file_paths:
            full_path = self.apm_root / file_path
            
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Extract key information
                analysis = {
                    "file_path": file_path,
                    "word_count": len(content.split()),
                    "line_count": len(content.splitlines()),
                    "categories": self.categorize_content(content),
                    "key_topics": self.extract_key_topics(content),
                    "integration_priority": self.assess_integration_priority(content, file_path),
                    "requires_indexing": True,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                
                analysis_results.append(analysis)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error analyzing {file_path}: {e}")
        
        return analysis_results
    
    def categorize_content(self, content):
        """Automatically categorize content based on keywords and patterns"""
        categories = []
        content_lower = content.lower()
        
        # Define category patterns
        category_patterns = {
            "robotics": ["robot", "ros", "moveit", "trajectory", "kinematics", "joint", "actuator"],
            "hardware": ["stm32", "arduino", "mcu", "sensor", "driver", "tmc", "stepper"],
            "software": ["python", "c++", "javascript", "algorithm", "framework", "api"],
            "analysis": ["analysis", "comparison", "evaluation", "study", "research"],
            "documentation": ["guide", "manual", "tutorial", "how-to", "installation"],
            "ai_ml": ["machine learning", "artificial intelligence", "neural", "model", "training"],
            "engineering": ["design", "cad", "manufacturing", "specifications", "standards"],
            "integration": ["integration", "bridge", "interface", "connector", "workflow"]
        }
        
        for category, keywords in category_patterns.items():
            if any(keyword in content_lower for keyword in keywords):
                categories.append(category)
        
        return categories if categories else ["general"]
    
    def extract_key_topics(self, content):
        """Extract key topics from content"""
        # Simple keyword extraction (could be enhanced with NLP)
        import re
        
        # Find words in headers and emphasized text
        headers = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)
        emphasized = re.findall(r'\*\*(.+?)\*\*', content)
        code_blocks = re.findall(r'`([^`]+)`', content)
        
        topics = []
        for header in headers[:5]:  # Top 5 headers
            topics.extend(header.lower().split()[:3])  # First 3 words
        
        for emp in emphasized[:10]:  # Top 10 emphasized terms
            if len(emp.split()) <= 2:  # Short phrases only
                topics.append(emp.lower())
        
        # Filter and clean topics
        topics = [t.strip() for t in topics if len(t) > 2 and t.isalnum()]
        return list(set(topics))[:10]  # Unique, max 10 topics
    
    def assess_integration_priority(self, content, file_path):
        """Assess integration priority based on content and location"""
        priority = "medium"
        
        # High priority indicators
        if any(term in content.lower() for term in ["production", "critical", "urgent", "important"]):
            priority = "high"
        elif any(term in file_path.lower() for term in ["active", "current", "main"]):
            priority = "high"
        
        # Low priority indicators  
        elif any(term in content.lower() for term in ["draft", "todo", "placeholder"]):
            priority = "low"
        elif any(term in file_path.lower() for term in ["archive", "old", "backup"]):
            priority = "low"
        
        return priority
    
    def update_indexes(self, knowledge_analyses):
        """Update all relevant indexes with new knowledge"""
        print("üìä Updating knowledge indexes...")
        
        # Update master index statistics
        self.update_master_index_stats(knowledge_analyses)
        
        # Update expansion index
        self.update_expansion_index(knowledge_analyses)
        
        # Update specialized indexes if they exist
        self.update_specialized_indexes(knowledge_analyses)
        
        print("‚úÖ Indexes updated successfully")
    
    def update_master_index_stats(self, analyses):
        """Update the master knowledge index with new statistics"""
        try:
            with open(self.master_index, 'r') as f:
                content = f.read()
            
            # Calculate updated statistics
            total_files = len(self.knowledge_state["file_checksums"])
            knowledge_docs = len([f for f in self.knowledge_state["file_checksums"] 
                                if f.endswith('.md')])
            
            # Update content overview section
            new_overview = f"""### **Content Overview** *(Auto-Updated {datetime.now().strftime('%Y-%m-%d')})*
- **Total Files Scanned:** {total_files}+ *(Auto-tracked with AI workflow)*
- **Knowledge Documents:** {knowledge_docs}+ *(Plus specialized analyses)*
- **Recent Additions:** {len(analyses)} new documents integrated
- **AI Workflow:** ‚úÖ Active automated integration
- **Last Integration:** {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}"""
            
            # Replace the content overview section
            import re
            pattern = r'### \*\*Content Overview\*\*.*?(?=###|\n## )'
            updated_content = re.sub(pattern, new_overview, content, flags=re.DOTALL)
            
            with open(self.master_index, 'w') as f:
                f.write(updated_content)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating master index: {e}")
    
    def update_expansion_index(self, analyses):
        """Update the knowledge expansion index"""
        try:
            # Generate new expansion index content
            timestamp = datetime.now(timezone.utc).isoformat()
            
            content = f"""# APM Knowledge Expansion Index
*Auto-Generated: {timestamp}*

## üìä **Current Knowledge Sources** *(AI Workflow Managed)*

### **üìö Documents & References**
- **Total Files**: {len(self.knowledge_state["file_checksums"])} auto-tracked
- **Recent Integrations**: {len(analyses)} new analyses
- **Knowledge Categories**: {self.get_category_stats()}
- **Integration Status**: ‚úÖ Fully automated workflow active
- **Quality Assurance**: All content validated and indexed

### **üöÄ Recent AI Workflow Integrations** *(Last 24 Hours)*
"""
            
            # Add recent integrations
            for analysis in analyses[-10:]:  # Last 10 integrations
                categories = ", ".join(analysis["categories"])
                content += f"- ‚úÖ **{Path(analysis['file_path']).name}**: {categories} ({analysis['word_count']} words)\n"
            
            content += f"""
### **ü§ñ AI Workflow Status**
- **Auto-Detection**: ‚úÖ Active file monitoring
- **Smart Analysis**: ‚úÖ Content categorization and topic extraction  
- **Index Updates**: ‚úÖ Automated maintenance
- **Git Integration**: ‚úÖ Automated commit and push
- **Quality Control**: ‚úÖ Validation and error handling

### **üìã Workflow Capabilities**
- Real-time knowledge acquisition detection
- Intelligent content analysis and categorization
- Automated documentation generation
- Index synchronization and maintenance
- Git workflow automation
- Integration status tracking
- Quality assurance validation

---

**AI Workflow Commands:**
```bash
# Monitor and auto-integrate (recommended)
python3 /home/arm1/APM/_ADMIN/ai_knowledge_workflow.py --mode auto

# Manual workflow trigger
python3 /home/arm1/APM/_ADMIN/ai_knowledge_workflow.py --mode manual

# Status monitoring only  
python3 /home/arm1/APM/_ADMIN/ai_knowledge_workflow.py --mode monitor
```
"""
            
            with open(self.expansion_index, 'w') as f:
                f.write(content)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error updating expansion index: {e}")
    
    def get_category_stats(self):
        """Get statistics about knowledge categories"""
        all_categories = []
        for file_info in self.knowledge_state.get("knowledge_items", []):
            all_categories.extend(file_info.get("categories", []))
        
        # Count unique categories
        category_counts = {}
        for cat in all_categories:
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        return f"{len(category_counts)} categories active"
    
    def update_specialized_indexes(self, analyses):
        """Update any specialized indexes that exist"""
        # Look for other index files and update them if needed
        index_files = list(self.apm_root.rglob("*INDEX*.md"))
        
        for index_file in index_files:
            if index_file != self.master_index and index_file != self.expansion_index:
                try:
                    # Add timestamp to specialized indexes
                    with open(index_file, 'r') as f:
                        content = f.read()
                    
                    # Add update timestamp at the end
                    timestamp_note = f"\n\n---\n*Auto-updated by AI Workflow: {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}*\n"
                    
                    if "Auto-updated by AI Workflow" not in content:
                        with open(index_file, 'a') as f:
                            f.write(timestamp_note)
                            
                except Exception as e:
                    print(f"‚ö†Ô∏è Error updating {index_file}: {e}")
    
    def validate_integration_quality(self, analyses):
        """Validate the quality of knowledge integration"""
        print("üîç Validating integration quality...")
        
        issues = []
        
        for analysis in analyses:
            file_path = self.apm_root / analysis["file_path"]
            
            # Check file exists and is readable
            if not file_path.exists():
                issues.append(f"Missing file: {analysis['file_path']}")
                continue
            
            # Check minimum content requirements
            if analysis["word_count"] < 10:
                issues.append(f"Insufficient content in: {analysis['file_path']}")
            
            # Check for proper markdown formatting
            try:
                with open(file_path, 'r') as f:
                    content = f.read()
                    if not content.strip().startswith('#'):
                        issues.append(f"Missing header in: {analysis['file_path']}")
            except Exception as e:
                issues.append(f"Read error in {analysis['file_path']}: {e}")
        
        if issues:
            print("‚ö†Ô∏è Quality issues detected:")
            for issue in issues:
                print(f"   ‚Ä¢ {issue}")
        else:
            print("‚úÖ All integrations passed quality validation")
        
        return len(issues) == 0
    
    def commit_and_push_changes(self, analyses):
        """Automatically commit and push changes via git"""
        print("üöÄ Committing and pushing changes...")
        
        try:
            # Change to APM directory
            os.chdir(self.apm_root)
            
            # Check git status
            result = subprocess.run(['git', 'status', '--porcelain'], 
                                  capture_output=True, text=True)
            
            if not result.stdout.strip():
                print("üìã No changes to commit")
                return True
            
            # Add all changes
            subprocess.run(['git', 'add', '.'], check=True)
            
            # Create comprehensive commit message
            commit_msg = self.generate_commit_message(analyses)
            
            # Commit changes
            subprocess.run(['git', 'commit', '-m', commit_msg], check=True)
            
            # Push to origin
            subprocess.run(['git', 'push', 'origin', 'master'], check=True)
            
            # Update push timestamp
            self.knowledge_state["last_push_timestamp"] = datetime.now(timezone.utc).timestamp()
            
            print("‚úÖ Changes committed and pushed successfully")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Git operation failed: {e}")
            return False
        except Exception as e:
            print(f"‚ùå Commit/push error: {e}")
            return False
    
    def generate_commit_message(self, analyses):
        """Generate a comprehensive commit message"""
        timestamp = datetime.now().strftime("%Y-%m-%d")
        
        # Categorize changes
        categories = set()
        high_priority = []
        
        for analysis in analyses:
            categories.update(analysis["categories"])
            if analysis["integration_priority"] == "high":
                high_priority.append(Path(analysis["file_path"]).name)
        
        # Build commit message
        msg_parts = [f"AI Workflow Auto-Integration: {timestamp}"]
        
        if high_priority:
            msg_parts.append(f"\nüö® High Priority Integrations:")
            for item in high_priority:
                msg_parts.append(f"   ‚Ä¢ {item}")
        
        msg_parts.append(f"\nüìä Knowledge Integration Summary:")
        msg_parts.append(f"   ‚Ä¢ {len(analyses)} new/updated documents")
        msg_parts.append(f"   ‚Ä¢ Categories: {', '.join(sorted(categories))}")
        msg_parts.append(f"   ‚Ä¢ Auto-indexed and validated")
        
        msg_parts.append(f"\n‚úÖ Automated Workflow Actions:")
        msg_parts.append(f"   ‚Ä¢ Content analysis and categorization")
        msg_parts.append(f"   ‚Ä¢ Index updates and synchronization")
        msg_parts.append(f"   ‚Ä¢ Quality validation passed")
        msg_parts.append(f"   ‚Ä¢ Git integration completed")
        
        msg_parts.append(f"\nü§ñ AI Workflow System: Fully automated knowledge lifecycle")
        
        return "".join(msg_parts)
    
    def run_auto_workflow(self):
        """Run the complete automated workflow"""
        print("ü§ñ AI Knowledge Workflow: AUTO MODE")
        print("=" * 50)
        
        try:
            # Step 1: Detect new knowledge
            new_files, modified_files = self.detect_new_knowledge()
            
            if not self.new_knowledge_detected:
                print("‚úÖ No new knowledge detected - system up to date")
                return True
            
            # Step 2: Analyze content
            all_files = new_files + modified_files
            analyses = self.analyze_knowledge_content(all_files)
            
            # Step 3: Update knowledge state
            self.knowledge_state["knowledge_items"].extend(analyses)
            
            # Step 4: Update indexes
            self.update_indexes(analyses)
            
            # Step 5: Validate integration quality
            if not self.validate_integration_quality(analyses):
                print("‚ùå Quality validation failed - manual review required")
                return False
            
            # Step 6: Commit and push
            if self.commit_and_push_changes(analyses):
                print("üéâ AI Knowledge Workflow completed successfully!")
                
                # Update workflow state
                self.knowledge_state["integration_status"] = "completed"
                self.save_workflow_state()
                
                return True
            else:
                print("‚ùå Git operations failed - manual intervention required")
                return False
                
        except Exception as e:
            print(f"‚ùå Workflow error: {e}")
            self.knowledge_state["integration_status"] = "error"
            self.save_workflow_state()
            return False
    
    def run_monitor_mode(self):
        """Monitor knowledge changes without taking action"""
        print("üëÅÔ∏è AI Knowledge Workflow: MONITOR MODE")
        print("=" * 50)
        
        new_files, modified_files = self.detect_new_knowledge()
        
        if self.new_knowledge_detected:
            print(f"üìã Changes detected but not processed (monitor mode)")
            print(f"   ‚Ä¢ New files: {len(new_files)}")
            print(f"   ‚Ä¢ Modified files: {len(modified_files)}")
            print(f"üí° Run with --mode auto to process changes")
        else:
            print("‚úÖ No changes detected - system synchronized")
        
        return True
    
    def run_manual_workflow(self):
        """Run workflow with user confirmation at each step"""
        print("üîß AI Knowledge Workflow: MANUAL MODE")
        print("=" * 50)
        
        # Detect changes
        new_files, modified_files = self.detect_new_knowledge()
        
        if not self.new_knowledge_detected:
            print("‚úÖ No changes detected")
            return True
        
        # Confirm processing
        response = input(f"üìã Process {len(new_files + modified_files)} changed files? (y/N): ")
        if response.lower() != 'y':
            print("‚è∏Ô∏è Workflow cancelled by user")
            return True
        
        # Continue with auto workflow
        return self.run_auto_workflow()

def main():
    """Main workflow function"""
    parser = argparse.ArgumentParser(description="AI Knowledge Workflow Automation")
    parser.add_argument('--mode', choices=['auto', 'manual', 'monitor'], 
                       default='auto', help='Workflow mode')
    
    args = parser.parse_args()
    
    workflow = AIKnowledgeWorkflow()
    
    if args.mode == 'auto':
        success = workflow.run_auto_workflow()
    elif args.mode == 'manual':
        success = workflow.run_manual_workflow()
    elif args.mode == 'monitor':
        success = workflow.run_monitor_mode()
    
    exit_code = 0 if success else 1
    exit(exit_code)

if __name__ == "__main__":
    main()